<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TechaScope</title>

  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #1e2b32;
      color: #0effe5;
      padding: 20px;
      text-align: center;
      display: flex;
      flex-direction: column;
      align-items: center;
      min-height: 100vh;
    }
    h1 {
      font-size: 3rem;
      margin-bottom: 20px;
      color: #00ffc3;
      text-shadow: 0 0 10px #00ffc3;
    }
    video, img {
      width: 100%;
      max-width: 500px;
      margin: 10px 0;
      border: 4px solid #0effe5;
      border-radius: 10px;
    }
    button {
      background-color: #00ffc3;
      color: #000;
      border: none;
      padding: 50px 100px;
      font-size: 4rem;
      border-radius: 15px;
      cursor: pointer;
      margin: 20px 0;
      box-shadow: 0 0 20px #00ffc3;
    }
    button:focus { outline: 4px solid #ff00ff; }
    textarea {
      width: 100%;
      max-width: 650px;
      height: 220px;
      margin-top: 30px;
      font-size: 1.7rem;
      border: 3px solid #0effe5;
      border-radius: 12px;
      padding: 15px;
      background-color: #111e24;
      color: #0effe5;
      box-shadow: 0 0 15px #00ffcc;
    }
    #serverResponse { margin-top: 15px; font-size: 1.2rem; color: #88fffe; }
    .container { display: flex; flex-direction: column; align-items: center; }
  </style>
</head>
<body>
  <h1 aria-label="Vision Assistant">TECHASCOPE</h1>

  <div class="container">
    <!-- Live camera -->
    <video id="video" autoplay playsinline aria-label="Live camera feed"></video>
    <button id="capturePhoto" aria-label="Capture photo manually">CAPTURE</button>

    <!-- Annotated result from server -->
    <h4>Annotated Result</h4>
    <img id="annotated" alt="annotated result will appear here" />

    <!-- Existing output (shows latest speech text too) -->
    <textarea id="ttsOutput" readonly aria-label="Spoken scene description"></textarea>
    <p id="serverResponse" aria-live="polite"></p>

    <!-- TTS controls (optional) -->
    <div style="margin-top:12px;">
      <button id="speakBtn" style="padding:10px 18px;font-size:1.2rem;">üîä Speak</button>
      <button id="stopBtn"  style="padding:10px 18px;font-size:1.2rem;">‚èπ Stop</button>
      <label style="margin-left:12px;">
        <input type="checkbox" id="autoSpeak" checked />
        Auto-speak after analysis
      </label>
    </div>

    <!-- New outputs matching your Python script -->
    <h4>Detected Object</h4>
    <p id="detectedObj"></p>

    <h4>Item Name</h4>
    <p id="itemName"></p>

    <h4>Speech Text</h4>
    <textarea id="speechText" readonly style="width:100%;height:120px;"></textarea>
  </div>

  <script>
    const video = document.getElementById('video');
    const captureBtn = document.getElementById('capturePhoto');
    const ttsOutput = document.getElementById('ttsOutput');
    const serverResponse = document.getElementById('serverResponse');
    const annotatedImg = document.getElementById('annotated');

    const detectedObjEl = document.getElementById('detectedObj');
    const itemNameEl = document.getElementById('itemName');
    const speechTextEl = document.getElementById('speechText');
    const speakBtn = document.getElementById('speakBtn');
    const stopBtn  = document.getElementById('stopBtn');
    const autoSpeakToggle = document.getElementById('autoSpeak');

    // --- Web Speech API helpers ---
    function speak(text) {
      if (!text || !('speechSynthesis' in window)) return;
      // cancel anything queued
      speechSynthesis.cancel();
      const utter = new SpeechSynthesisUtterance(text);
      // Optional tuning
      utter.rate = 1.0;   // 0.1..10
      utter.pitch = 1.0;  // 0..2
      utter.volume = 1.0; // 0..1
      // Pick an English voice if available (best-effort)
      const voices = speechSynthesis.getVoices();
      const en = voices.find(v => /en(-|_)?(US|GB|AU|CA|IN)?/i.test(v.lang));
      if (en) utter.voice = en;
      speechSynthesis.speak(utter);
    }

    function stopSpeaking() {
      if ('speechSynthesis' in window) speechSynthesis.cancel();
    }

    // Buttons
    if (speakBtn) speakBtn.onclick = () => {
      // prefer your combined speech text; fall back to ttsOutput
      const text = (document.getElementById('speechText')?.value || '').trim()
                || (document.getElementById('ttsOutput')?.value || '').trim();
      speak(text);
    };
    if (stopBtn)  stopBtn.onclick = stopSpeaking;

    // Ensure voices are loaded (some browsers populate them asynchronously)
    if ('speechSynthesis' in window) {
      speechSynthesis.onvoiceschanged = () => speechSynthesis.getVoices();
    }

    let fileBlob = null;
    let stream = null;

    async function startCamera() {
      try {
        stream = await navigator.mediaDevices.getUserMedia({
          video: { facingMode: 'environment' }, // rear camera on phones if available
          audio: false
        });
        video.srcObject = stream;

        // Wait until video has dimensions to avoid black frames
        await new Promise(resolve => {
          if (video.readyState >= 2 && video.videoWidth) return resolve();
          video.onloadedmetadata = () => resolve();
        });

        // Optional auto-capture every 12.5s
        startAutoCapture();
      } catch (err) {
        alert("Camera access error: " + err.message);
      }
    }

    function captureImage() {
      if (!video.videoWidth || !video.videoHeight) return;
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d', { willReadFrequently: true });
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      canvas.toBlob(blob => {
        fileBlob = blob;
        sendToServer();
      }, 'image/jpeg', 0.92);
    }

    function startAutoCapture() {
      setInterval(captureImage, 12500); // every 12.5 seconds
    }

    captureBtn.addEventListener('click', captureImage);

    async function sendToServer() {
      if (!fileBlob) {
        serverResponse.textContent = 'No image captured.';
        return;
      }
      serverResponse.textContent = 'Sending‚Ä¶';

      const formData = new FormData();
      formData.append('file', fileBlob, 'photo.jpg');
      // If you add a confidence control later, append it here:
      // formData.append('conf', '0.5');

      try {
        // Use RELATIVE URL so this works in any environment
        const response = await fetch('/upload', { method: 'POST', body: formData });
        if (!response.ok) throw new Error('HTTP ' + response.status);
        const result = await response.json();

        // Annotated image (base64 jpeg)
        if (result.annotated_image) {
          annotatedImg.src = 'data:image/jpeg;base64,' + result.annotated_image;
        }

        // New fields mirroring your Image_Recognition.py
        detectedObjEl.textContent = result.detected_object || '(none)';
        itemNameEl.textContent = result.item_name || '(none)';
        speechTextEl.value = (result.speech_text || '').trim();

        // Keep your original output text area in sync too
        ttsOutput.value = (result.speech_text || result.ocr_text || '').trim();

        // Auto-speak if enabled
        const toSpeak = speechTextEl.value || ttsOutput.value;
        if (autoSpeakToggle && autoSpeakToggle.checked && toSpeak) {
          speak(toSpeak);
        }

        serverResponse.textContent = 'Server Response: ' + (result.message || 'ok');
        console.log('Objects:', result.objects || []);
      } catch (error) {
        console.error('Error sending data:', error);
        serverResponse.textContent = 'Error sending data. Make sure the server is running.';
      }
    }

    // Start camera on load
    startCamera();

    // Stop camera on page unload
    window.addEventListener('beforeunload', () => {
      if (stream) stream.getTracks().forEach(t => t.stop());
    });
  </script>
</body>
</html>
